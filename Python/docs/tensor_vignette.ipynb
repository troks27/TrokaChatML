{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Decomposition Analysis with `trokachat_tensor_decomp`\n",
    "\n",
    "This notebook demonstrates how to use the `trokachat_tensor_decomp` package to analyze tensor data from two experimental conditions, estimate the optimal tensor rank, and perform tensor decomposition. It replicates the analysis from the original script but uses the new modular package structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Modules\n",
    "\n",
    "First, let's import the necessary modules from our package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import functions from our package\n",
    "from trokachat_tensor_decomp import (\n",
    "    load_tensor_data,\n",
    "    create_sparse_tensor,\n",
    "    calc_denoising_loss_parallel,\n",
    "    plot_mse_boxplot,\n",
    "    perform_decomposition,\n",
    "    save_decomposition_results\n",
    ")\n",
    "\n",
    "# Define a helper function for saving dimension mappings since it's not in the package yet\n",
    "def save_dimension_mappings(dim_mappings, output_dir):\n",
    "    \"\"\"Save dimension mappings to CSV files\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    mapping_files = []\n",
    "    \n",
    "    for i, mapping in enumerate(dim_mappings):\n",
    "        mapping_df = pd.DataFrame(list(mapping.items()), columns=[f'Category_{i}', 'Index'])\n",
    "        mapping_file_path = os.path.join(output_dir, f\"mapping_{i}.csv\")\n",
    "        mapping_df.to_csv(mapping_file_path, index=False)\n",
    "        mapping_files.append(mapping_file_path)\n",
    "        \n",
    "    return mapping_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Process Tensor Data\n",
    "\n",
    "Let's load the tensor data from the CSV files using the exact same file paths from the original script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths from the original script\n",
    "tensor_data_path = \"/Users/troks27/Desktop/DIAB_NG_TrokaChatML/TrokaChat/2 Condition Tensor/2condition_tensor_data.csv\"\n",
    "tensor_dimensions_path = \"/Users/troks27/Desktop/DIAB_NG_TrokaChatML/TrokaChat/2 Condition Tensor/2condition_tensor_dimensions.csv\"\n",
    "\n",
    "# Define output directories\n",
    "output_dir = \"/Users/troks27/Desktop/DIAB_NG_TrokaChatML/TrokaChat/Decomposed Tensor/\"\n",
    "rank_results_dir = \"/Users/troks27/Desktop/DIAB_NG_TrokaChatML/TrokaChat/Tensor Rank Determination/\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(rank_results_dir, exist_ok=True)\n",
    "\n",
    "# Load tensor data using our package function\n",
    "tensor_data, tensor_dimensions = load_tensor_data(tensor_data_path, tensor_dimensions_path)\n",
    "\n",
    "# Display information about the data\n",
    "print(f\"Loaded tensor data with {len(tensor_data)} rows\")\n",
    "print(\"Columns in tensor data:\")\n",
    "print(tensor_data.columns)\n",
    "tensor_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Sparse Tensor\n",
    "\n",
    "Now, we'll convert the DataFrame to a sparse tensor representation using our package function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dimension columns (same as in the original script)\n",
    "dimension_columns = ['Source', 'Target', 'Ligand_Receptor', 'Condition']\n",
    "\n",
    "# Create sparse tensor\n",
    "sparse_tl_tensor, tensor_shape, dim_mappings = create_sparse_tensor(tensor_data, dimension_columns)\n",
    "\n",
    "# Display information about the tensor\n",
    "print(f\"Created sparse tensor with shape: {tensor_shape}\")\n",
    "print(f\"Number of non-zero elements: {sparse_tl_tensor.nnz}\")\n",
    "sparse_tl_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Estimate Tensor Rank\n",
    "\n",
    "We'll determine the optimal tensor rank through denoising loss calculation for different noise levels. This replicates the analysis from the original script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same parameters as in the original script\n",
    "rank_max = 30\n",
    "noise_levels = [0.05, 0.1, 0.2, 0.3, 0.4]\n",
    "n_rep = 20\n",
    "\n",
    "# We'll just run one noise level for demonstration\n",
    "# In practice, you'd run all noise levels as in the original script\n",
    "sample_noise = 0.1  # Choose a sample noise level for demonstration\n",
    "\n",
    "print(f\"Estimating tensor rank with noise level {sample_noise}...\")\n",
    "avg_errors, err_summary_df = calc_denoising_loss_parallel(\n",
    "    sparse_tl_tensor, \n",
    "    rank_max=rank_max, \n",
    "    noise=sample_noise, \n",
    "    n_rep=n_rep,\n",
    "    method=\"MSE\"\n",
    ")\n",
    "\n",
    "# Save the results\n",
    "csv_filename = f\"{rank_results_dir}denoising_mask_noise={sample_noise}_rank_estim.csv\"\n",
    "err_summary_df.to_csv(csv_filename, index=False)\n",
    "print(f\"Saved rank estimation results to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Rank Estimation Results\n",
    "\n",
    "Let's visualize the rank estimation results using our package's visualization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot for the sample noise level\n",
    "output_filename = f\"{rank_results_dir}denoising_mask_noise={sample_noise}_rank_estim_plot.pdf\"\n",
    "plot_mse_boxplot(csv_filename=csv_filename, output_filename=output_filename)\n",
    "\n",
    "# Plot the average errors\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, rank_max + 1), avg_errors, marker='o')\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Rank\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(f\"Average Loss vs Rank (Noise = {sample_noise})\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find the optimal rank\n",
    "optimal_rank = np.argmin(avg_errors) + 1\n",
    "print(f\"Optimal rank based on minimum loss: {optimal_rank}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Process All Noise Levels\n",
    "\n",
    "In a full analysis, you would process all noise levels and create plots for each one. Here's how you would do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell shows how to process all noise levels\n",
    "# We're commenting it out to avoid long computation\n",
    "\n",
    "'''\n",
    "for noise in noise_levels:\n",
    "    print(f\"\\nProcessing noise level: {noise}\")\n",
    "    \n",
    "    # Run the denoising loss calculation\n",
    "    avg_errors, err_summary_df = calc_denoising_loss_parallel(\n",
    "        sparse_tl_tensor, \n",
    "        rank_max=rank_max, \n",
    "        noise=noise, \n",
    "        n_rep=n_rep\n",
    "    )\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    csv_filename = f\"{rank_results_dir}denoising_mask_noise={noise}_rank_estim.csv\"\n",
    "    err_summary_df.to_csv(csv_filename, index=False)\n",
    "    \n",
    "    # Create and save the plot\n",
    "    output_filename = f\"{rank_results_dir}denoising_mask_noise={noise}_rank_estim_plot.pdf\"\n",
    "    plot_mse_boxplot(csv_filename=csv_filename, output_filename=output_filename)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load and Visualize Existing Results\n",
    "\n",
    "If you've already run the analysis and saved the results, you can load and visualize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all noise levels from previously saved results\n",
    "for noise in noise_levels:\n",
    "    csv_filename = f'{rank_results_dir}denoising_mask_noise={noise}_rank_estim.csv'\n",
    "    if os.path.exists(csv_filename):\n",
    "        output_filename = f'{rank_results_dir}denoising_mask_noise={noise}_rank_estim_plot.pdf'\n",
    "        plot_mse_boxplot(csv_filename=csv_filename, output_filename=output_filename)\n",
    "        print(f\"Created plot for noise level {noise}\")\n",
    "    else:\n",
    "        print(f\"File not found: {csv_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Perform Tensor Decomposition\n",
    "\n",
    "Based on the rank estimation, we'll perform tensor decomposition with the optimal rank. In the original script, rank 16 was chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the rank for decomposition (same as in the original script)\n",
    "rank = 16  # Based on rank estimation from the original analysis\n",
    "\n",
    "# Perform non-negative CP decomposition using our package function\n",
    "print(f\"Performing tensor decomposition with rank {rank}...\")\n",
    "factors = perform_decomposition(\n",
    "    sparse_tl_tensor, \n",
    "    rank=rank, \n",
    "    init='random', \n",
    "    verbose=True, \n",
    "    n_iter_max=1000\n",
    ")\n",
    "\n",
    "print(\"Tensor decomposition completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Decomposition Results\n",
    "\n",
    "Now, we'll save the decomposition results (weights and factors) using our package function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dimension mappings\n",
    "mapping_files = save_dimension_mappings(dim_mappings, output_dir)\n",
    "print(f\"Saved dimension mappings to {output_dir}\")\n",
    "\n",
    "# Save decomposition results\n",
    "saved_files = save_decomposition_results(factors, output_dir)\n",
    "\n",
    "print(f\"Weights saved to: {saved_files.get('weights')}\")\n",
    "print(f\"Factor matrices saved to:\")\n",
    "for factor_file in saved_files.get('factors', []):\n",
    "    print(f\"  - {factor_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analyze Decomposition Results\n",
    "\n",
    "Let's analyze the decomposition results to understand the patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract weights and factors\n",
    "weights, factors_list = factors\n",
    "\n",
    "# Convert weights to dense array\n",
    "weights_array = np.array(weights.todense()).flatten()\n",
    "\n",
    "# Plot weights\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(1, len(weights_array) + 1), weights_array)\n",
    "plt.xlabel('Component')\n",
    "plt.ylabel('Weight')\n",
    "plt.title('Component Weights')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Analyze the factor matrices\n",
    "dimension_names = ['Source', 'Target', 'Ligand_Receptor', 'Condition']\n",
    "\n",
    "for i, factor in enumerate(factors_list):\n",
    "    print(f\"\\nAnalyzing Factor {i} ({dimension_names[i]})\")\n",
    "    print(f\"Shape: {factor.shape}\")\n",
    "    \n",
    "    # Convert to dense if needed\n",
    "    if hasattr(factor, 'todense'):\n",
    "        factor_dense = factor.todense()\n",
    "    else:\n",
    "        factor_dense = factor\n",
    "    \n",
    "    # Plot heatmap for this factor\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(factor_dense, cmap='viridis')\n",
    "    plt.xlabel('Component')\n",
    "    plt.ylabel(f'{dimension_names[i]} Index')\n",
    "    plt.title(f'Factor Matrix for {dimension_names[i]}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # For the condition factor (which is small), display the values\n",
    "    if i == 3:  # Condition factor\n",
    "        factor_df = pd.DataFrame(factor_dense, columns=[f'Component_{j+1}' for j in range(rank)])\n",
    "        # Map indices to actual condition names\n",
    "        reverse_mapping = {v: k for k, v in dim_mappings[i].items()}\n",
    "        factor_df.index = [reverse_mapping.get(idx, idx) for idx in range(len(factor_df))]\n",
    "        print(\"Condition factor values:\")\n",
    "        display(factor_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Examine Component Contributions\n",
    "\n",
    "Let's identify the top contributors for each component across the different dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get top contributors for a factor and component\n",
    "def get_top_contributors(factor_matrix, component_idx, dimension_mapping, top_n=10):\n",
    "    # Convert to dense if needed\n",
    "    if hasattr(factor_matrix, 'todense'):\n",
    "        factor_dense = factor_matrix.todense()\n",
    "    else:\n",
    "        factor_dense = factor_matrix\n",
    "    \n",
    "    # Get the values for this component\n",
    "    component_values = factor_dense[:, component_idx]\n",
    "    \n",
    "    # Create a DataFrame with indices and values\n",
    "    df = pd.DataFrame({\n",
    "        'Index': range(len(component_values)),\n",
    "        'Value': component_values\n",
    "    })\n",
    "    \n",
    "    # Sort by value in descending order\n",
    "    df = df.sort_values('Value', ascending=False)\n",
    "    \n",
    "    # Get the top N contributors\n",
    "    top_contributors = df.head(top_n)\n",
    "    \n",
    "    # Map indices to actual names\n",
    "    reverse_mapping = {v: k for k, v in dimension_mapping.items()}\n",
    "    top_contributors['Name'] = top_contributors['Index'].map(reverse_mapping)\n",
    "    \n",
    "    return top_contributors[['Name', 'Value']]\n",
    "\n",
    "# Analyze a specific component (e.g., component 0)\n",
    "component_to_analyze = 0\n",
    "\n",
    "print(f\"\\nAnalyzing Component {component_to_analyze + 1}\")\n",
    "print(\"------------------------------\")\n",
    "\n",
    "for i, factor in enumerate(factors_list):\n",
    "    print(f\"\\nTop contributors in {dimension_names[i]}:\")\n",
    "    top_contributors = get_top_contributors(factor, component_to_analyze, dim_mappings[i], top_n=5)\n",
    "    display(top_contributors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to use the `trokachat_tensor_decomp` package to:\n",
    "\n",
    "1. Load tensor data from CSV files\n",
    "2. Create a sparse tensor representation\n",
    "3. Estimate the optimal tensor rank using denoising loss calculation\n",
    "4. Visualize the rank estimation results\n",
    "5. Perform tensor decomposition with the selected rank\n",
    "6. Save and analyze the decomposition results\n",
    "\n",
    "The package provides a clean, modular interface to tensor decomposition operations, making the analysis workflow more readable and maintainable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
